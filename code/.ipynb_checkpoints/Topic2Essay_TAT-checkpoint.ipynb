{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:10.302540Z",
     "start_time": "2018-11-11T02:46:09.896847Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, autograd, optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:24.058823Z",
     "start_time": "2018-11-11T02:46:10.303820Z"
    }
   },
   "outputs": [],
   "source": [
    "fvec = KeyedVectors.load_word2vec_format('sgns.weibo.word', binary=False)\n",
    "word_vec = fvec.vectors\n",
    "vocab = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']\n",
    "vocab.extend(list(fvec.vocab.keys()))\n",
    "word_vec = np.concatenate((np.array([[0]*word_vec.shape[1]] * 4), word_vec))\n",
    "word_vec = torch.tensor(word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:24.104116Z",
     "start_time": "2018-11-11T02:46:24.060384Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx_to_word = {i: ch for i, ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:28.473706Z",
     "start_time": "2018-11-11T02:46:24.105745Z"
    }
   },
   "outputs": [],
   "source": [
    "essays = []\n",
    "topics = []\n",
    "with open('processed_data.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        essay, topic = line.replace('\\n', '').split(' </d> ')\n",
    "        essays.append(essay.split(' '))\n",
    "        topics.append(topic.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:28.548569Z",
     "start_time": "2018-11-11T02:46:28.475013Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_indice = list(map(lambda x: [word_to_idx[w] for w in x], essays[:8000]))\n",
    "topics_indice = list(map(lambda x: [word_to_idx[w] for w in x], topics[:8000]))\n",
    "corpus_test = list(map(lambda x: [word_to_idx[w] for w in x], essays[8000:8800]))\n",
    "topics_test = list(map(lambda x: [word_to_idx[w] for w in x], topics[8000:8800]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:28.617760Z",
     "start_time": "2018-11-11T02:46:28.549666Z"
    }
   },
   "outputs": [],
   "source": [
    "length = list(map(lambda x: len(x), corpus_indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:28.700082Z",
     "start_time": "2018-11-11T02:46:28.619988Z"
    }
   },
   "outputs": [],
   "source": [
    "def tav_data_iterator(corpus_indice, topics_indice, batch_size, num_steps):\n",
    "    epoch_size = len(corpus_indice) // batch_size\n",
    "    for i in range(epoch_size):\n",
    "        raw_data = corpus_indice[i*batch_size: (i+1)*batch_size]\n",
    "        key_words = topics_indice[i*batch_size: (i+1)*batch_size]\n",
    "        data = np.zeros((len(raw_data), num_steps+1), dtype=np.int64)\n",
    "        for i in range(batch_size):\n",
    "            doc = raw_data[i]\n",
    "            tmp = [1]\n",
    "            tmp.extend(doc)\n",
    "            tmp.extend([2])\n",
    "            tmp = np.array(tmp, dtype=np.int64)\n",
    "            _size = tmp.shape[0]\n",
    "            data[i][:_size] = tmp\n",
    "        key_words = np.array(key_words, dtype=np.int64)\n",
    "        x = data[:, 0:num_steps]\n",
    "        y = data[:, 1:]\n",
    "        mask = np.float32(x != 0)\n",
    "        x = torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "        mask = torch.tensor(mask)\n",
    "        key_words = torch.tensor(key_words)\n",
    "        yield(x, y, mask, key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:28.823710Z",
     "start_time": "2018-11-11T02:46:28.701402Z"
    }
   },
   "outputs": [],
   "source": [
    "class TATLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, embed_dim, num_layers, weight,\n",
    "                 num_labels, bidirectional, dropout=0.5, **kwargs):\n",
    "        super(TATLSTM, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_labels = num_labels\n",
    "        self.bidirectional = bidirectional\n",
    "        if num_layers <= 1:\n",
    "            self.dropout = 0\n",
    "        else:\n",
    "            self.dropout = dropout\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.rnn = nn.GRU(input_size=self.embed_dim, hidden_size=self.hidden_dim,\n",
    "                          num_layers=self.num_layers, bidirectional=self.bidirectional,\n",
    "                          dropout=self.dropout)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(hidden_dim * 2 + self.embed_dim, self.num_labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(hidden_dim + self.embed_dim, self.num_labels)\n",
    "        self.attn = nn.Linear(self.embed_dim * 5, self.embed_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, topics, hidden=None):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        topics_embed = self.embedding(topics).float()\n",
    "        topics_attn = self.attn(topics_embed.reshape((topics_embed.shape[0], -1)))\n",
    "        topics_attn.unsqueeze_(-1)\n",
    "#         for i in range(embeddings.shape[0]):\n",
    "#             embeddings[i][0] = topics_embed[i]\n",
    "        states, hidden = self.rnn(embeddings.permute([1, 0, 2]).float(), hidden)\n",
    "        topics_attn = topics_attn.expand(topics_attn.shape[0], topics_attn.shape[1], states.shape[0])\n",
    "        topics_attn = topics_attn.permute([2, 0, 1])\n",
    "        states_with_topic = torch.cat([states, topics_attn], dim=2)\n",
    "        outputs = self.decoder(states_with_topic.reshape((-1, states_with_topic.shape[-1])))\n",
    "        return(outputs, hidden)\n",
    "    \n",
    "    def init_hidden(self, num_layers, batch_size, hidden_dim, **kwargs):\n",
    "        hidden = torch.zeros(num_layers, batch_size, hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:28.930880Z",
     "start_time": "2018-11-11T02:46:28.827904Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_rnn(topics, num_chars, model, device, idx_to_word, word_to_idx):\n",
    "    output = [1]\n",
    "    topics = [word_to_idx[x] for x in topics]\n",
    "    topics = torch.tensor(topics)\n",
    "    topics = topics.reshape((1, topics.shape[0], -1))\n",
    "    hidden = torch.zeros(num_layers, 1, hidden_dim)\n",
    "    if use_gpu:\n",
    "        hidden = hidden.to(device)\n",
    "        topics = topics.to(device)\n",
    "    for t in range(num_chars):\n",
    "        X = torch.tensor(output[-1]).reshape((1, 1))\n",
    "#         X = torch.tensor(output).reshape((1, len(output)))\n",
    "        if use_gpu:\n",
    "            X = X.to(device)\n",
    "        pred, hidden = model(X, topics, hidden)\n",
    "        if pred.argmax(dim=1)[-1] == 2:\n",
    "            break\n",
    "        else:\n",
    "            output.append(int(pred.argmax(dim=1)))\n",
    "#             output.append(int(pred.argmax(dim=1)[-1]))\n",
    "    return(''.join([idx_to_word[i] for i in output[1:]]), output[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:29.038949Z",
     "start_time": "2018-11-11T02:46:28.935042Z"
    }
   },
   "outputs": [],
   "source": [
    "def bleu(pred_tokens, label_tokens, k):\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k+1):\n",
    "        num_matches = 0\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if ' '.join(pred_tokens[i: i + n]) in ' '.join(label_tokens):\n",
    "                num_matches += 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:29.119663Z",
     "start_time": "2018-11-11T02:46:29.043049Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "lr = 1e2\n",
    "momentum = 0.0\n",
    "num_epoch = 100\n",
    "use_gpu = True\n",
    "num_layers = 1\n",
    "bidirectional = False\n",
    "batch_size = 8\n",
    "verbose = 5\n",
    "device = torch.device('cuda:0')\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:31.223716Z",
     "start_time": "2018-11-11T02:46:29.123786Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TATLSTM(hidden_dim=hidden_dim, embed_dim=embedding_dim, num_layers=num_layers,\n",
    "                num_labels=len(vocab), weight=word_vec, bidirectional=bidirectional)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "if use_gpu:\n",
    "#     model = nn.DataParallel(model)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:46:32.020322Z",
     "start_time": "2018-11-11T02:46:31.225097Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'近视不除揣上溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也溜之乎也'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn(['变形金刚','三星级'], 100, model, device, idx_to_word, word_to_idx)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T07:55:31.003000Z",
     "start_time": "2018-11-11T02:46:32.021409Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.30it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5/100, loss 5.9458, norm 0.7823, bleu: 0.0093, time 137.402s, since 0h 11m 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人们的人们都会喜欢我的美丽，我也喜欢我最喜欢我的美丽，我也喜欢我最喜欢我的美丽，我也喜欢我的美丽，我也喜欢我的美丽，我也喜欢我的美丽，我也喜欢我的美丽，我也喜欢我的美丽，我也喜欢我的美丽，我也喜欢我的美丽，我也喜欢我的美丽，我也喜欢我的美丽，我也喜欢我的美丽，\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.29it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.25it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10/100, loss 5.0203, norm 0.9469, bleu: 0.1399, time 137.403s, since 0h 24m 51s\n",
      "人们的人们都会喜欢人们的美丽。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:16,  7.28it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1it [00:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15/100, loss 4.3566, norm 1.1449, bleu: 0.0067, time 137.421s, since 0h 46m 41s\n",
      "人们的美丽的美丽。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1it [00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20/100, loss 3.8430, norm 1.8767, bleu: 0.0000, time 137.539s, since 1h 0m 6s\n",
      "人们的美丽的笑脸。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1it [00:00,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25/100, loss 3.4498, norm 1.4331, bleu: 0.0935, time 137.555s, since 1h 12m 3s\n",
      "的人们。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:16,  7.29it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.25it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30/100, loss 3.1546, norm 1.3691, bleu: 0.0389, time 137.497s, since 1h 33m 54s\n",
      "秋天来了，夏天的夏天，人们都会喜欢自己的人。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1it [00:00,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35/100, loss 2.9591, norm 1.4780, bleu: 0.0001, time 137.582s, since 1h 49m 23s\n",
      "的人们。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.25it/s]\n",
      "1000it [02:17,  7.25it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40/100, loss 2.8253, norm 1.7286, bleu: 0.0428, time 137.564s, since 2h 2m 10s\n",
      "秋姑娘又来人们带来了无穷的的秋景。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.25it/s]\n",
      "1000it [02:17,  7.25it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45/100, loss 2.7306, norm 1.5368, bleu: 0.0030, time 137.588s, since 2h 19m 25s\n",
      "秋姑娘又来了，他们在欢迎人们的到来，我最喜欢的是美丽的秋景。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50/100, loss 2.6637, norm 1.5730, bleu: 0.0110, time 137.627s, since 2h 32m 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "秋姑娘的到来，人们也会感受到了夏天的到来，我要喜欢这是秋景的秋景，我爱秋天，因为人们能感受到夏天的寒冷人们带来人们的美丽。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.25it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55/100, loss 2.6071, norm 2.0138, bleu: 0.0188, time 137.630s, since 2h 46m 53s\n",
      "人们的人们都是在秋中的生活中，人们都会感受到到夏天的到来到来到来。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.29it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.24it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60/100, loss 2.5658, norm 1.6831, bleu: 0.0625, time 137.728s, since 3h 0m 36s\n",
      "人们喜欢的春天，这种也是夏天的炎热。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65/100, loss 2.5415, norm 1.8972, bleu: 0.0179, time 137.717s, since 3h 17m 39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "秋天来了，人们都在歌唱着人们的到来，人们在欢迎着，人们都在歌唱着歌唱，在人们的眼中，他们都在歌唱着，歌唱着，人们说：“欢迎，欢迎，我要要喜欢这！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.25it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70/100, loss 2.5185, norm 1.4968, bleu: 0.0000, time 137.796s, since 3h 32m 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "秋天来了，夏阿姨去了果园里，我又听到了：“我要要看看看看我呀！”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.24it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.25it/s]\n",
      "1000it [02:17,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75/100, loss 2.5103, norm 2.2964, bleu: 0.0000, time 137.828s, since 3h 44m 19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "秋天来了，夏天的雨里，人们都在歌唱着人们的到来。我要看了，好多人都会歌唱着。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.23it/s]\n",
      "1000it [02:17,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80/100, loss 2.5091, norm 1.7077, bleu: 0.0083, time 137.814s, since 3h 56m 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "秋天到了，雨中的人们都觉得很难，但我却觉得自己的骄傲，反而也要闻到了。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.29it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.25it/s]\n",
      "1000it [02:17,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85/100, loss 2.5159, norm 2.4245, bleu: 0.0982, time 137.747s, since 4h 10m 32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "秋天来了，秋天来了，夏天的炎热，人们都在人们的眼中中，人们尽情地享受着丰收的果实。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90/100, loss 2.5474, norm 2.4875, bleu: 0.0002, time 137.778s, since 4h 28m 19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "秋天的西瓜成熟了，苹果也要穿上了美丽的秋景，人们都在歌唱着跳舞的秋景。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.29it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.24it/s]\n",
      "1000it [02:17,  7.26it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95/100, loss 2.5836, norm 4.1016, bleu: 0.0929, time 137.747s, since 4h 42m 15s\n",
      "秋天的雨，是人们的歌唱。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:17,  7.26it/s]\n",
      "1000it [02:17,  7.28it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.27it/s]\n",
      "1000it [02:17,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100/100, loss 2.6567, norm 2.2180, bleu: 0.0536, time 137.511s, since 5h 4m 6s\n",
      "秋天的雨真是美丽的。它虽然不喜欢，夏天的炎热，我要喜欢它，它也不像夏天一样炎热的夏天，但我觉得自己的一切都是一派平凡的景象。\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "for epoch in range(num_epoch):\n",
    "    start = time.time()\n",
    "    num, total_loss = 0, 0\n",
    "#     if epoch == 5000:\n",
    "#         optimizer.param_groups[0]['lr'] = lr * 0.1\n",
    "    data = tav_data_iterator(\n",
    "        corpus_indice, topics_indice, batch_size, max(length) + 1)\n",
    "    hidden = model.init_hidden(num_layers, batch_size, hidden_dim)\n",
    "    weight = torch.ones(len(vocab))\n",
    "    weight[0] = 0\n",
    "    for X, Y, mask, topics in tqdm(data):\n",
    "        num += 1\n",
    "        hidden.detach_()\n",
    "        if use_gpu:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            mask = mask.to(device)\n",
    "            topics = topics.to(device)\n",
    "            hidden = hidden.to(device)\n",
    "            weight = weight.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(X, topics, hidden)\n",
    "        l = F.cross_entropy(output, Y.t().reshape((-1,)), weight)\n",
    "        l.backward()\n",
    "        norm = nn.utils.clip_grad_norm_(model.parameters(), 1e-2)\n",
    "        optimizer.step()\n",
    "        total_loss += l.item()\n",
    "    end = time.time()\n",
    "    s = end - since\n",
    "    h = math.floor(s / 3600)\n",
    "    m = s - h * 3600\n",
    "    m = math.floor(m / 60)\n",
    "    s -= (m * 60 + h * 3600)\n",
    "    if((epoch + 1) % verbose == 0) or (epoch == (num_epoch - 1)):\n",
    "        bleu_score = 0\n",
    "        for i in range(len(corpus_test)):\n",
    "            doc = corpus_test[i]\n",
    "            _, pred = predict_rnn([idx_to_word[x] for x in topics_test[0]],\n",
    "                                  100, model, device, idx_to_word, word_to_idx)\n",
    "            bleu_score += bleu([idx_to_word[int(x)] for x in pred],\n",
    "                               [idx_to_word[x] for x in doc if x not in [0, 2]], k=2)\n",
    "        print('epoch %d/%d, loss %.4f, norm %.4f, predict bleu: %.4f, time %.3fs, since %dh %dm %ds'\n",
    "              % (epoch + 1, num_epoch, total_loss / num, norm, bleu_score / 800, end - start, h, m, s))\n",
    "        print(predict_rnn(['变形金刚','三星级'],\n",
    "                          100, model, device, idx_to_word, word_to_idx)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T07:55:31.115251Z",
     "start_time": "2018-11-11T07:55:31.012987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从前的松树围着着五彩缤纷的的的湖面。如果湖面上，有几片火红的松树。有松树绣在小草，有的半开围着，有的像小姑娘在捉迷藏。有的松树还是那么惹人。\n"
     ]
    }
   ],
   "source": [
    "print(''.join([idx_to_word[x] for x in pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T10:08:00.687625Z",
     "start_time": "2018-11-11T10:08:00.684320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鸟儿 动听 中午 郁郁葱葱 看着\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([idx_to_word[x] for x in topics_test[len(topics_test) - 1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
